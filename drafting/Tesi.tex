\documentclass[
	12pt,
	a4paper,
	cleardoublepage=empty,
	headings=twolinechapter,
	numbers=autoenddot,
]{scrbook}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{import}
\usepackage{float}
\usepackage{cite}
\usepackage{fancyhdr}
\usepackage[signatures,swapnames,sans]{frontespizio}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{todonotes}
\usepackage{verbatim}

\usepackage{color}
\usepackage{url} 
\usepackage{caption}
\usepackage{subcaption}
\usepackage[hidelinks]{hyperref}

\newcommand{\Fig}[0]{Fig.}
\newcommand{\Tab}[0]{Tab.}
\newcommand{\Eq}[0]{Eq.}
\newcommand{\Def}[0]{Def.}
\newcommand{\Chap}[0]{Chap.}

\usepackage[standardsections]{scrhack}
\usepackage[raggedright]{titlesec}

\titleformat{\chapter}[display]
{\bfseries\LARGE}
{\filright{\huge\chaptertitlename} \huge\thechapter}
{2ex}
{\titlerule\vspace{0.8ex}\filright}
[\vspace{0.8ex}\titlerule]

\newtheorem{definition}{Definition}[section]
\newcounter{thm}
\newtheorem{dfn}[thm]{Definition}

\pagestyle{plain}
%\input{preamble}
%\newcolumntype{C}{>{\centering\arraybackslash}p{3cm}}
\linespread{1.5}

\begin{document}
	\frontmatter
	
	%\pdfbookmark{Title page}{titlepage}
	\begin{frontespizio}
		\Margini{3cm}{3cm}{3cm}{3cm}
		\Universita{Bergamo}
		\Logo[43.332mm]{ImageFiles/unibg-mark}
		\Divisione{Scuola di Ingegneria}
		\Corso[Laurea Triennale]{Corso di Laurea Magistrale in Ingegneria Informatica\\Classe N. LM-32 â€“ Classe delle lauree magistrali in Ingegneria Informatica}
		\Titolo{Evaluating the Robustness of Bayesian Neural Networks by Exploiting Uncertainty Estimation}
		\Punteggiatura{}
		\NCandidato{Tesi di Laurea Magistrale}
		\Candidato[1060652]{Wasim Essbai}
		\Relatore{Prof.\ Angelo Gargantini}
		\Correlatore{Dott.\ Andrea Bombarda}
		\Correlatore{Dott.ssa\ Silvia Bonfanti}
		\Annoaccademico{2022--2023}
		\begin{Preambolo*}
			\usepackage[english]{babel}
			\usepackage[T1]{fontenc}
			\usepackage[utf8]{inputenc}
			\usepackage{microtype}
			\graphicspath{{img/}}
			
			\renewcommand{\frontinstitutionfont}{\fontsize{14}{17}\bfseries\scshape}
			\renewcommand{\fronttitlefont}{\fontsize{17}{21}\bfseries\scshape}
			\renewcommand{\frontfootfont}{\fontsize{12}{14}\bfseries\scshape}
		\end{Preambolo*}
	\end{frontespizio}
	
	\tableofcontents
	\listoffigures
	\mainmatter
	
	\chapter*{Abstract}
	\addcontentsline{toc}{chapter}{Abstract} 
	\import{./TextFiles/}{Abstract.tex}
	
	\chapter{Introduction}
	\import{./TextFiles/}{Introduction.tex}
	
	\chapter{Theoretical background}
	This chapter will cover the fundamental theoretical concepts used in this study. Specifically, we will start by introducing neural networks (NNs), starting from their basic building blocks and progressing to the training phase. Following that, it will be explored a way to improve NNs using a Bayesian approach.
	
	\import{./TextFiles/Theoretical background/}{NN.tex}
	\import{./TextFiles/Theoretical background/}{BNN.tex}
		
	\chapter{Case study}
	This work aims to examine the robustness of a BNN when applied to a classification task on images. Specifically, there will be developed an ANN and a BNN to compare their performances and determine if utilizing the BNN enhances robustness.
	
	The networks will be implemented in \textit{Python 3.9} as the programming language, utilizing the framework \textit{Pytorch} \footnote{It is a Python-based framework that allows the development of ML models at a high level. It provides support for GPU programming, allowing users to easily leverage the computational power of GPUs to reduce the computational time.}\cite{NEURIPS2019_9015}. 
	For the training and all the evaluations, \textit{Google Colaboratory} will be utilized as the platform. For the implementation of the BNN the library \textit{torchbnn}\footnote{It is a library for the development of BNNs using Pytorch.}\cite{Torchbnn} will be used.
	
	\import{./TextFiles/Case study/}{Dataset.tex}
	\import{./TextFiles/Case study/}{Network architecture.tex}
	\import{./TextFiles/Case study/}{Training.tex}
	
	\chapter{Classification using uncertainty}\label{chap:c3}
	
	This chapter introduces a novel classification approach that leverages the uncertainty estimated by a BNN. The primary objective is to assess the practical utility of this estimated uncertainty. The idea behind this approach is to enable the network to identify situations where it lacks confidence in its predictions, thus allowing the system to avoid from making decisions. To achieve this, a threshold for uncertainty will be defined, and whenever the uncertainty is below this threshold, the network will simply output "I don't know". 
	The proposed method explores three distinct types of uncertainty. The first two types, epistemic and aleatoric uncertainties, have been previously discussed and remain consistent in their significance across different tasks. The third type corresponds to the variance of the predicted distribution, which might have a distinct interpretation and could potentially lead to different threshold as well. The threshold value must be proportional to the desired predictions confidence. In this study the threshold is defined using a linear interpolation between the maximum and minimum uncertainty levels, formally:
	\[
		threshold = maxUnc * (1 - conf \_ level), \ with \ con \_ level \in [0,1]
	\]
	where $maxUnc$ is the maximum uncertainty that can occur and $conf_{level}$ is the confidence level, with $1$ representing the maximum.

	When classification with uncertainty is performed the definition of accuracy changes in order to highlight the number of correct and unknown predictions.
	
	\begin{definition} (Extended accuracy).
		Given a classifier $C$ and a set of input $D$, the accuracy of $C$ on $D$ is defined as:
		\[
			acc(C,D) = \frac{|{d \in D | C(d) = correctLabel(d)}|}{|D| - |{d \in D | C(d) = unknownLabel}|}
		\] 
		where $C(d)$ represents the classification output for the input $p$, and $correctLabel(d)$ gives the correct label for the input $d$.
	\end{definition}
	
	However, some applications may need to make decisions at all times. Therefore, an alternative approach will be explored where classification is performed by penalizing the most uncertain output classes. This allows for decision-making even when uncertainty is present.
	
	\import{./TextFiles/Classification uncertainty/}{Aleatoric.tex}
	\import{./TextFiles/Classification uncertainty/}{Epistemic.tex}
	\import{./TextFiles/Classification uncertainty/}{Deviation.tex}
	\import{./TextFiles/Classification uncertainty/}{Penalization.tex}
	\import{./TextFiles/Classification uncertainty/}{Comparison.tex}
	
	\chapter{Robustness}\label{chap:c4}
	
	This chapter provides an overview of the current state of the art in testing machine learning models (MLMs). These approaches are applicable to a wide range of MLMs, including BNNs, which are the central focus of this thesis. Moreover, a novel approach for testing and evaluating BNNs will be introduced. The primary emphasis will be on assessing the robustness of BNNs, which is a crucial property in safety-critical applications.
	
	\import{./TextFiles/Robustness/}{StateOfArt.tex}
	
	\import{./TextFiles/Robustness/}{RobNN.tex}
	\import{./TextFiles/Robustness/}{RobBNN.tex}
	
	\chapter{Robustness evaluation}\label{chap:c5}
	
	This chapter illustrates the practical application of the methods derived in the previous chapters, presenting experimental outcomes. To do this, a testing framework will be introduced, designed to be adaptable to any other MLM. These methodologies are founded on a black-box testing approach, wherein the framework assesses the model performance when exposed to perturbed operating conditions. In this study, the primary source of perturbations under consideration is the data itself. Hence, a set of potential alterations that could occur in a real-world application will be applied to the test data to simulate a perturbed environment.
	
	\import{./TextFiles/RobustnessEval/}{TestFramework.tex}
	\import{./TextFiles/RobustnessEval/}{Alterations.tex}
	\import{./TextFiles/RobustnessEval/}{ExperimentalResults.tex}
	
	\chapter{Conclusion}
	
	In the course of our work, we developed and tested innovative techniques for assessing the robustness of NNs, which can be applied broadly to any MLM. Subsequently, the focus was put on MLMs which provide uncertainty estimations, specifically BNNs. This required us to incorporate uncertainty into the computed metrics. To accomplish this, we introduced the concept of \textit{effectiveness}, which measures performance in terms of both accuracy and unknown ratio.
	
	In conclusion, our investigation into the assessment of BNNs robustness has revealed interesting insights into their performance. When employed in a conventional manner, BNNs exhibit a level of robustness comparable to that of standard NNs. However, the true potential of BNNs in enhancing the robustness emerges when uncertainty is introduced into the classification process.
	
	Our findings highlight that BNNs are particularly suitable in situations characterized by ambiguity and unpredictability. By incorporating uncertainty into their computations, BNNs can effectively identify and respond to situations where the model confidence may be compromised, unlike standard NNs which tend to provide steady predictions. This inherent capability to recognize uncertainty and adapt accordingly grants BNNs a distinct advantage in scenarios where making incorrect predictions could have significant consequences. This behavior is particularly desirable in safety-critical applications, such as the medical field, where it is preferable to halt the system and request intervention rather than making incorrect predictions. Nevertheless, BNNs can also prove beneficial in recognizing uncertain situations and executing appropriate procedures without halting the system.
	
	In summary, our research underlies the significance of incorporating BNNs into machine learning frameworks, particularly in domains where robustness is paramount. By embracing uncertainty, BNNs not only match the robustness of standard NNs but surpass them in recognizing and handling uncertain conditions, thereby showcasing their potential to enhance the reliability and trustworthiness of machine learning systems in real-world applications. Based on these considerations, it is evident that BNNs can be a way for creating more resilient and autonomous artificial intelligence systems.
	
	\import{./TextFiles/Conclusion/}{FutureWork.tex}
	
	\bibliographystyle{ieeetr}
	
	\bibliography{./OtherFiles/Bibliography}
	
	\addcontentsline{toc}{chapter}{Bibliography}
	
\end{document}