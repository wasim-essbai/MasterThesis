Artificial intelligence software is increasingly finding its way into critical domains, with many of these software applications relying on machine learning models. The concept behind these algorithms is to \textit{learn} directly from data, enhancing their performance through statistical methods.

The recent advancement in electronics hardware have enabled the development of intricate models capable of solving complex problems, such as image classification. Machine learning algorithms, especially deep neural networks, have demonstrated remarkable effectiveness across a wide range of tasks. However, their main vulnerability are uncertainty, adversarial attacks, and data distribution shifts. 

The consequences of unreliable predictions in critical domains such as healthcare, autonomous systems, and finance can be profound, accentuating the need for models that not only excel in performance but also understand and communicate their own limitations.
Given that these models learn directly from data and autonomously adjust their parameters, it becomes challenging to understand their internal functioning.

Bayesian Neural Networks (BNNs) have emerged as a promising solution to address this critical need. Unlike their deterministic counterparts, BNNs provide a principled framework for modeling uncertainty in neural networks predictions. They treat network weights as probabilistic variables, enabling them to capture uncertainty in both model parameters and predictions. This intrinsic uncertainty estimation capability offers a promising opportunity to evaluate and enhance model robustness by exploiting the wealth of information encapsulated within it.

This thesis embarks on a journey to explore the evaluation of the robustness of BNNs through the exploitation of uncertainty estimation. It aims to address fundamental questions concerning the role of uncertainty in machine learning and its practical implications.

This project is a research output carried out within the \textit{``Talent Improvement Program"} at the \textit{University of Bergamo}. The principal aim of this research endeavor was the development of novel methodologies for testing machine learning models using black-box-based approaches.